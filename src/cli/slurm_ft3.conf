# slurm.conf file maintained by HPCNow!
#
SlurmctldHost=adm211-11
SlurmctldHost=adm211-12
#ControlAddr=localhost
#
#
#
# ------------ Generic Slurm Configuration -----------------
# Useful for implementing rolling updates, i.e. patch and restart.
DisableRootJobs=YES
EnforcePartLimits=ALL
#FirstJobId=1
#MaxJobId=999999
GresTypes=gpu
#GroupUpdateForce=0
#GroupUpdateTime=600
#JobFileAppend=0
JobRequeue=0
#KillOnBadExit=0
#LaunchType=launch/slurm
#MailProg=/bin/mail
#
# Update the following values to support HTC needs
#MaxJobCount=5000
#MaxStepCount=40000
#MaxTasksPerNode=128
#
# Leave this value as none by default
# OpenMPI is compiled with PMIx with EasyBuild
# Intel MPI module loads the variable: I_MPI_PMI_LIBRARY=/usr/lib64/libpmi.so
# More information here: https://hpcnow.atlassian.net/wiki/spaces/TD/pages/534052911/MPI
MpiDefault=none
#MpiParams=ports=#-#
#PluginDir=
#PlugStackConfig=
#
# Update this parameter to controls what type of information
# is hidden from regular users.
#PrivateData=accounts,jobs,users
#
# Esta opción también es necesaria añadirla slurmdbd.conf
#
PrivateData=accounts,jobs,reservations,usage,users
ProctrackType=proctrack/cgroup
#ProctrackType=proctrack/linuxproc
#PropagatePrioProcess=0
PropagateResourceLimits=NONE
#PropagateResourceLimitsExcept=
#RebootProgram=
ReturnToService=0
SlurmctldPidFile=/var/run/slurm/slurmctld.pid
SlurmctldPort=6810-6817
SlurmdPidFile=/var/run/slurm/slurmd.pid
SlurmdPort=6818
SlurmdUser=root
### The folder must be accessible by the primary and backup control machines.
#StateSaveLocation=/var/spool/slurm/ctld
StateSaveLocation=/HA/slurmctld
SwitchType=switch/none
TaskPlugin=task/affinity,task/cgroup
#TmpFS=/tmp
#TrackWCKey=no
# To be set up for large clusters. Optimal system performance can typically be achieved if TreeWidth is set to the square root of the number of nodes in the cluster for systems having no more than 2500 nodes or the cube root for larger systems.
# Review the Large Cluster Administration documentation: https://slurm.schedmd.com/big_sys.html
#TreeWidth=
#
#UnkillableStepProgram=
#UsePAM=0
#
#
#
# ------------ HPCNow! Topology Aware Scheduling -----------
# Use the script slurmibtopology.sh located in scripts/sbin folder to
# auto-populate the topology.conf file.
# If it fails, check for a newer release on GitHub
# https://github.com/OleHolmNielsen/Slurm_tools/blob/master/slurmibtopology/
# IMPORTANT: Consider updating the parameter max_switch_wait in SchedulerParameters
# The max_switch_wait is extremely important for fat-tree topology with high blocking factor (i.e. 4 or 8).
TopologyPlugin=topology/tree
#
#
#
# ------------ HPCNow! Job Submit Plugins ------------------
# Multiple custom job submit plugins available in "job_submit" folder
# In order to install custom plugins, extend the list below and
# transfer a copy in the main slurm configuration folder with the
# following name: /etc/slurm/job_submit.lua
#
# Other relevant plugins:
# all_partitions: sets default partition to all partitions on the cluster and routes the jobs to the partitions that meet the requirements.
# This pluting relies on the partition priority to route the jobs. If using preemption and you want to avoid medium jobs to preempt long jobs, you will need a custom partition routing plugin.
# require_timelimit: force job requests to include time limit
# submit_throttle*: limits the number of job submissions that any single user can make. The limits are defined by the parameter: SchedulerParameters=jobs_per_user_per_hour=#
JobSubmitPlugins=require_timelimit,lua
#JobSubmitPlugins=all_partitions,require_timelimit,lua
#
#
#
# ------------ HPCNow! License Management ------------------
# FlexLM Support is documented in flexlm folder
# To review undocumented repository
# https://bitbucket.org/hpcnow/license-control/src/master/
#Licenses=foo*4,bar
#
#
#
# ------------ HPCNow! Prolog and Epilog -------------------
# Review if Slurm has been compiled with X11 support.
PrologFlags=X11
PrologEpilogTimeout=300
#EpilogSlurmctld=
#PrologSlurmctld=
# IMPORTANT: Update the paths in the prolog and epilog scripts <-- to review PrEpPlugins
Epilog=/etc/slurm/epilog/job.sh
Prolog=/etc/slurm/prolog/job.sh
#SrunEpilog=/etc/slurm/epilog/srun.sh
#SrunProlog=/etc/slurm/prolog/srun.sh
#TaskEpilog=/etc/slurm/epilog/task.sh
TaskProlog=/etc/slurm/prolog/task.sh
#
#
#
#
# ------------ HPCNow! Checkpointing & Restart -------------
# Support for DMTCP/MANA to be documented
# https://hpcnow.atlassian.net/wiki/spaces/TD/pages/1670447105/DMTCP+-+MANA+Support
#CheckpointType=checkpoint/none
# The following folder must be exposed via a very fast cluster file system.
#JobCheckpointDir=/scratch/checkpoint
#
#
#
# ------------ HPCNow! Timers ------------------------------
# LBNL Node Health Check (NHC) documentation available here: https://github.com/mej/nhc
# Configuration can be auto-generated with:
# pdsh -a "/usr/sbin/nhc-genconf -H '*' -c -" | dshbak -c
# The nhc.conf-template file contains additional options to be considered.
HealthCheckInterval=300
HealthCheckProgram=/usr/sbin/nhc
#
#BatchStartTimeout=10
# To minimize fragmentation of resources, a value equal to KillWait plus two is recommended.
# Consider to increase these values if jobs are crashing due to slow cluster file system or rogue tasks
CompleteWait=32
KillWait=30
#EpilogMsgTime=2000
#GetEnvTimeout=2
# Limit for inactive sessions
InactiveLimit=300
# Increase this value to mitigate network congestion, daemon being page out, or other side effects.
MessageTimeout=30
# No need to setup this value anymore as reservations allow to overrun the limit
# if instructed with flag: NO_HOLD_JOBS_AFTER_END
# More information here: https://bitbucket.org/hpcnow/slurm-administration/src/master/09-resource-reservation.md
# Setting up reservations with "Purging After Last Job" are strongly suggested. Flag: PURGE_COMP
#ResvOverRun=0
MinJobAge=300
#OverTimeLimit=0
#
# This value is reduced because this implementation relies on Corosync and Pacemaker for HA
SlurmctldTimeout=30
SlurmdTimeout=300
#UnkillableStepTimeout=60
#VSizeFactor=0
Waittime=0
#
#
# ------------ HPCNow! Advanced Scheduling -----------------
#
# Use preempt/qos when using preemption based on the PartitionQoS priorities.
# Preemption based on requeue seems to work only with preempt/partition_prio.
# More information available in the internal training
# https://bitbucket.org/hpcnow/slurm-administration/src/master/08-advanced-scheduling-strategies.md
#
# Valores iniciales para usar preemption
#
#PreemptType=preempt/partition_prio
#PreemptMode=suspend,gang
PreemptMode=OFF
PreemptType=preempt/none

# Nos quedamos aqui
#
DefMemPerCPU=1024
#MaxMemPerCPU=0
#SchedulerTimeSlice=30
SchedulerType=sched/backfill
SelectType=select/cons_tres
SelectTypeParameters=CR_Core_Memory
#
# Parameters to be reviewed here: https://slurm.schedmd.com/sched_config.html
# Important parameters to reduce the stress/load of Slurmctld
# defer (avoids attempting to schedule each job individually at job submit time, it will be done later time when scheduling multiple jobs simultaneously may be possible). This option improves system responsiveness in HTC/large clusters.
# bf_continue (the backfill scheduler will continue processing pending jobs from its original job list after releasing locks.)
# bf_interval (number of seconds between backfill iterations). Default: 30, Min: 1, Max: 10800 (3h).
# bf_resolution (number of seconds in the resolution of data maintained about when jobs begin and end). Default 60, the higher, the more responsiveness.
# bf_window (number of minutes into the future to look when considering jobs to schedule). The higher the values, the more overhead. A value at least as long as the highest allowed time limit is generally advisable to prevent job starvation. Default 1440 (1 day).
#
# Others
# max_switch_wait (maximum number of seconds that a job can delay execution waiting for the specified desired switch count). Default 5 minutes, HPCNow! value 30 minutes.
# 
# FT-2 parameters:
# SchedulerParameters=bf_continue,bf_interval=30,bf_max_job_test=3000,bf_max_job_user=10,bf_resolution=1800,bf_window=10080,default_queue_depth=10000,defer,kill_invalid_depend,max_depend_depth=1,max_rpc_cnt=150,max_switch_wait=864000,nohold_on_prolog_fail
#
# Initial values
#SchedulerParameters=defer,bf_continue,bf_interval=60,bf_resolution=300,bf_window=10080,bf_busy_nodes,default_queue_depth=1000,bf_max_job_start=200,bf_max_job_test=2000,max_switch_wait=1800
# Increase of some parameters (bf_interval=90????)
SchedulerParameters=defer,bf_continue,bf_interval=60,bf_resolution=300,bf_window=10080,bf_busy_nodes,default_queue_depth=10000,bf_max_job_start=0,bf_max_job_test=6000,max_switch_wait=1800,bf_max_job_user=20
# Increase of some parameters (bf_interval=90????)  ------- WITHOUT bf_max_job_user -------
#SchedulerParameters=defer,bf_continue,bf_interval=60,bf_resolution=300,bf_window=10080,bf_busy_nodes,default_queue_depth=10000,bf_max_job_start=0,bf_max_job_test=6000,max_switch_wait=1800


#
#
# ------------ HPCNow! Job Priority ------------------------
# HPCNow! usually enables the following two flags:
# DEPTH_OBLIVIOUS: The priority is calculated based similar to the normal multifactor calculation, but depth of the associations in the tree do not adversely effect their priority.
# SMALL_RELATIVE_TO_TIME: The job's size component is based on the job's size divided by its time limit. This provides a more fair competition.
# CALCULATE_RUNNING: The priorities are recalculated not only for pending jobs, but also running and suspended jobs. Interesting to enable it for job preemption (suspension).
PriorityFlags=DEPTH_OBLIVIOUS,SMALL_RELATIVE_TO_TIME
PriorityType=priority/multifactor
PriorityDecayHalfLife=0
#PriorityCalcPeriod=
PriorityFavorSmall=NO
# Avoid enabling age priority when using Fairsharing in order to keep it fair.
PriorityMaxAge=15-00:00:00
#
# At this interval the usage of associations will be reset to 0. This is used if you want to enforce hard limits of time usage per association.
# By default this is turned off and it is advised to use the PriorityDecayHalfLife option to avoid not having anything running on your cluster.
# If your schema is set up to only allow certain amounts of time on your system this is the way to do it.
PriorityUsageResetPeriod=YEARLY
PriorityWeightAge=10000
PriorityWeightAssoc=10000
PriorityWeightFairshare=10000
PriorityWeightJobSize=100000
PriorityWeightPartition=2000
PriorityWeightQOS=10000
#
#
# ------------ HPCNow! Logging and Accounting --------------
AccountingStorageEnforce=safe,associations,limits,qos
AccountingStorageHost=10.120.0.102
#AccountingStoragePass=PASSWORD
#AccountingStoragePort=
AccountingStorageType=accounting_storage/slurmdbd
#AccountingStorageUser=slurm
ClusterName=finisterrae3
#DebugFlags=
#JobCompHost=
#JobCompLoc=elasticsearch:9200
#JobCompPass=
#JobCompPort=
#JobCompType=jobcomp/elasticsearch
#JobCompUser=
#JobContainerType=job_container/none
JobAcctGatherFrequency=30
JobAcctGatherType=jobacct_gather/cgroup
SlurmctldDebug=3
SlurmctldLogFile=/var/log/slurm/slurmctld.log
SlurmdDebug=3
#DebugFlags=Energy
#SlurmdDebug=debug2
SlurmdLogFile=/var/log/slurm/slurmd.log
SlurmSchedLogFile=/var/log/slurm/slurm_scheduler.log
SlurmSchedLogLevel=1
#
#
# ------------ HPCNow! Power Support -----------------------
#SuspendProgram=
#ResumeProgram=
#SuspendTimeout=
#ResumeTimeout=
#ResumeRate=
#SuspendExcNodes=
#SuspendExcParts=
#SuspendRate=
#SuspendTime=
#
#
# ------------ HPCNow! Compute Nodes Configuration ---------
include /etc/slurm/nodes.conf
# ------------ HPCNow! Partitions Configuration ------------
include /etc/slurm/partitions.conf

# ------------ CESGA Track gpu TRES ------------------------
AccountingStorageTRES=gres/gpu

# ------------ CESGA Active KillOnBadExit ------------------
KillOnBadExit=1

# ------------ CESGA kill_invalid_depend is deprecated in SchedulerParameters and moved to DependencyParameters ------------------
DependencyParameters=kill_invalid_depend

# ------------ CESGA Need to use option SLURM_RESV_PORTS (same as --resv-ports)
MpiParams=ports=10000-20000

# ------------ CESGA Enable energy gather (#54478)
#AcctGatherNodeFreq=30
#AcctGatherEnergyType=acct_gather_energy/ipmi

# ------------ CESGA Increase UnkillableStepTimeout
UnkillableStepTimeout=180

# ------------ CESGA Need to submit job arrays with large index
MaxArraySize=10001
MaxJobCount=100000

