# Ele slurm.conf file. Please run configurator.html
# (in doc/html) to build a configuration file customized
# for your environment.
#
#
# slurm.conf file generated by configurator.html.
# Put this file on all nodes of your cluster.
# See the slurm.conf man page for more information.
#
ClusterName=qmio
SlurmctldHost=admin01
SlurmctldHost=admin02
#SlurmctldHost=
#
#DisableRootJobs=NO
EnforcePartLimits=ALL
#Epilog=
#EpilogSlurmctld=
#FirstJobId=1
#MaxJobId=67043328
#GresTypes=
#GroupUpdateForce=0
#GroupUpdateTime=600
#JobFileAppend=0
JobRequeue=0
#JobSubmitPlugins=lua
#KillOnBadExit=1
#LaunchType=launch/slurm
#Licenses=foo*4,bar
#MailProg=/bin/mail
#MaxJobCount=10000
#MaxStepCount=40000
#MaxTasksPerNode=512
MpiDefault=none
#MpiParams=ports=#-#
#PluginDir=
#PlugStackConfig=
#PrivateData=jobs
#ProctrackType=proctrack/linuxproc
#Prolog=
#PrologFlags=
#PrologSlurmctld=
#PropagatePrioProcess=0
#PropagateResourceLimits=
#PropagateResourceLimitsExcept=
#RebootProgram=
SlurmctldPidFile=/var/run/slurmctld.pid
SlurmctldPort=6817
SlurmdPidFile=/var/run/slurmd.pid
SlurmdPort=6818
SlurmdSpoolDir=/var/spool/slurmd
SlurmUser=slurm
#SlurmdUser=root
#SrunEpilog=
#SrunProlog=
#StateSaveLocation=/var/spool/slurmctld
StateSaveLocation=/Slurm-MDB/slurmctld
SwitchType=switch/none
#TaskEpilog=
#TaskPlugin=task/affinity
#TaskProlog=
#TopologyPlugin=topology/tree
#TmpFS=/tmp
#TrackWCKey=no
#TreeWidth=
#UnkillableStepProgram=
#UsePAM=0
#
#
# TIMERS
#BatchStartTimeout=10
#CompleteWait=0
#EpilogMsgTime=2000
#GetEnvTimeout=2
#HealthCheckInterval=0
#HealthCheckProgram=
#InactiveLimit=0
KillWait=30
#MessageTimeout=10
#ResvOverRun=0
MinJobAge=300
#OverTimeLimit=0
SlurmctldTimeout=120
SlurmdTimeout=300
#UnkillableStepTimeout=60
#VSizeFactor=0
Waittime=0
#
#
# SCHEDULING
#DefMemPerCPU=0
#MaxMemPerCPU=0
#SchedulerTimeSlice=30
SchedulerType=sched/backfill
SelectType=select/cons_tres
SelectTypeParameters=CR_Core_Memory
#
#
# JOB PRIORITY
#PriorityFlags=
#PriorityType=priority/basic
#PriorityDecayHalfLife=
#PriorityCalcPeriod=
#PriorityFavorSmall=
#PriorityMaxAge=
#PriorityUsageResetPeriod=
#PriorityWeightAge=
#PriorityWeightFairshare=
#PriorityWeightJobSize=
#PriorityWeightPartition=
#PriorityWeightQOS=
#
#
# LOGGING AND ACCOUNTING
#AccountingStorageEnforce=0
AccountingStorageHost=admin01
#AccountingStoragePass=
#AccountingStoragePort=
AccountingStorageType=accounting_storage/slurmdbd
#AccountingStorageUser=
#AccountingStoreFlags=
#JobCompHost=
#JobCompLoc=/var/log/slurm/slurm_jobcomp.log
#JobCompPass=
#JobCompPort=
#JobCompType=jobcomp/none
#JobCompUser=
#JobContainerType=job_container/none
JobAcctGatherFrequency=30
#JobAcctGatherType=jobacct_gather/linux
SlurmctldDebug=verbose
SlurmctldLogFile=/var/log/slurm/slurmctld.log
SlurmdDebug=verbose
SlurmdLogFile=/var/log/slurm/slurmd.log
#SlurmSchedLogFile=
#SlurmSchedLogLevel=
#DebugFlags=
#
#
# POWER SAVE SUPPORT FOR IDLE NODES (optional)
#SuspendProgram=
#ResumeProgram=
#SuspendTimeout=
#ResumeTimeout=
#ResumeRate=
#SuspendExcNodes=
#SuspendExcParts=
#SuspendRate=
#SuspendTime=
#
#
# COMPUTE NODES
# OpenHPC default configuration
#TaskPlugin=task/affinity
#PropagateResourceLimitsExcept=MEMLOCK
#DependencyParameters=kill_invalid_depend
#JobCompType=jobcomp/filetxt
#Epilog=/etc/slurm/slurm.epilog.clean
Epilog=/etc/slurm/epilog/epilog.sh
#
#   Esta es la opcion para que no sea necesario tener
# la configuracion de slurm en los nodos
#
SlurmctldParameters=enable_configless
#ReturnToService=1

HealthCheckProgram=/usr/sbin/nhc
HealthCheckInterval=300

Prolog=/etc/slurm/prolog/job.sh 
TaskProlog=/etc/slurm/prolog/task.sh
#PrologSlurmctld=/etc/slurm/prolog/slurmctld_prolog.sh

#
# ------------ CESGA Changes -----------
#
AccountingStorageEnforce=safe,associations,limits,qos
DefMemPerCPU=1024
DisableRootJobs=YES
# Limit for inactive sessions
InactiveLimit=300
JobAcctGatherType=jobacct_gather/cgroup
JobSubmitPlugins=lua,require_timelimit
# Increase this value to mitigate network congestion, daemon being page out, or other side effects.
MessageTimeout=30
PreemptMode=OFF
PreemptType=preempt/none
# ------------ Job Priority ------------------------
# usually enables the following two flags:
# DEPTH_OBLIVIOUS: The priority is calculated based similar to the normal multifactor calculation, but depth of the associations in the tree do not adversely effect their priority.
# SMALL_RELATIVE_TO_TIME: The job's size component is based on the job's size divided by its time limit. This provides a more fair competition.
# CALCULATE_RUNNING: The priorities are recalculated not only for pending jobs, but also running and suspended jobs. Interesting to enable it for job preemption (suspension).
# MAX_TRES: If set, the weighted TRES value (e.g. TRESBillingWeights) is calculated as the MAX of individual TRES' on a node (e.g. cpus, mem, gres) plus the sum of all global TRES' (e.g. licenses) 
PriorityFlags=DEPTH_OBLIVIOUS,SMALL_RELATIVE_TO_TIME,MAX_TRES
PriorityType=priority/multifactor
PriorityDecayHalfLife=0
#PriorityCalcPeriod=
PriorityFavorSmall=NO
# Avoid enabling age priority when using Fairsharing in order to keep it fair.
PriorityMaxAge=15-00:00:00
#
# At this interval the usage of associations will be reset to 0. This is used if you want to enforce hard limits of time usage per association.
# By default this is turned off and it is advised to use the PriorityDecayHalfLife option to avoid not having anything running on your cluster.
# If your schema is set up to only allow certain amounts of time on your system this is the way to do it.
PriorityUsageResetPeriod=YEARLY
PriorityWeightAge=10000
PriorityWeightAssoc=10000
PriorityWeightFairshare=10000
PriorityWeightJobSize=1000
PriorityWeightPartition=500
PriorityWeightQOS=20000
# Update this parameter to controls what type of information
# is hidden from regular users.
#
# Esta opción también es necesaria añadirla slurmdbd.conf
#
# Initially users cannot view all jobs but it was requested
PrivateData=accounts,jobs,reservations,usage,users
#PrivateData=accounts,reservations,usage,users
ProctrackType=proctrack/cgroup
# Review if Slurm has been compiled with X11 support.
PrologFlags=X11
PrologEpilogTimeout=300
PropagateResourceLimits=NONE
ReturnToService=0
#
# Parameters to be reviewed here: https://slurm.schedmd.com/sched_config.html
# Important parameters to reduce the stress/load of Slurmctld
# defer (avoids attempting to schedule each job individually at job submit time, it will be done later time when scheduling multiple jobs simultaneously may be possible). This option improves system responsiveness in HTC/large clusters.
# bf_continue (the backfill scheduler will continue processing pending jobs from its original job list after releasing locks.)
# bf_interval (number of seconds between backfill iterations). Default: 30, Min: 1, Max: 10800 (3h).
# bf_resolution (number of seconds in the resolution of data maintained about when jobs begin and end). Default 60, the higher, the more responsiveness.
# bf_window (number of minutes into the future to look when considering jobs to schedule). The higher the values, the more overhead. A value at least as long as the highest allowed time limit is generally advisable to prevent job starvation. Default 1440 (1 day).
#
# Others
# max_switch_wait (maximum number of seconds that a job can delay execution waiting for the specified desired switch count). Default 5 minutes, HPCNow! value 30 minutes.
## CONFIG INICIAL SIMILAR AL FT3
#SchedulerParameters=defer,bf_continue,bf_interval=60,bf_resolution=300,bf_window=10080,bf_busy_nodes,default_queue_depth=1000,bf_max_job_start=200,bf_max_job_test=2000,max_switch_wait=1800
## BAJAMOS ALGUNOS PARAMETROS AL ESPERARSE MENOS TRABAJOS
SchedulerParameters=defer,bf_continue,bf_interval=30,bf_resolution=300,bf_window=10080,bf_busy_nodes,default_queue_depth=1000,bf_max_job_start=200,bf_max_job_test=2000,max_switch_wait=1800,salloc_wait_nodes,sbatch_wait_nodes

SlurmSchedLogFile=/var/log/slurm/slurm_scheduler.log
SlurmSchedLogLevel=1
TaskPlugin=task/affinity,task/cgroup
TopologyPlugin=topology/tree

# ------------ CESGA Compute Nodes Configuration ---------
include /etc/slurm/nodes.conf
# ------------ CESGA Partitions Configuration ------------
include /etc/slurm/partitions.conf

# ------------ CESGA Track gpu TRES ------------------------
AccountingStorageTRES=gres/gpu

# ------------ CESGA Active KillOnBadExit ------------------
KillOnBadExit=1

# ------------ CESGA kill_invalid_depend is deprecated in SchedulerParameters and moved to DependencyParameters ------------------
DependencyParameters=kill_invalid_depend

# ------------ CESGA Need to use option SLURM_RESV_PORTS (same as --resv-ports)
MpiParams=ports=10000-20000

# ------------ CESGA Enable energy gather (#54478)
#AcctGatherNodeFreq=30
#AcctGatherEnergyType=acct_gather_energy/ipmi

# ------------ CESGA Increase UnkillableStepTimeout
UnkillableStepTimeout=180

# ------------ CESGA Need to submit job arrays with large index
MaxArraySize=10001
MaxJobCount=100000

# ------------ CESGA POWER SAVE SUPPORT FOR IDLE NODES
SuspendExcNodes=c7-[101-108]
SuspendExcParts=qpu,ilk_interactive
SuspendExcStates=down,drain,fail,maint,not_responding,planned,reserved
ResumeTimeout=600
# SuspendTime=3600   # Global value: the default is INFINITE.  DO NOT change the default!
SuspendTimeout=120
ResumeProgram=/etc/slurm/power_save/noderesume
ResumeFailProgram=/etc/slurm/power_save/nodefailresume
SuspendProgram=/etc/slurm/power_save/nodesuspend
#DebugFlags=Power
TreeWidth=1000
ReconfigFlags=KeepPowerSaveSettings

